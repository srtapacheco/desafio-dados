# ğŸ“Š Desafio de Cientista de Dados - AnÃ¡lise e Tratamento de Dados

## ğŸ“Œ DescriÃ§Ã£o do Projeto
Este repositÃ³rio contÃ©m a soluÃ§Ã£o do **Desafio de Cientista de Dados**, que consiste na **anÃ¡lise, estruturaÃ§Ã£o e transformaÃ§Ã£o de dados brutos** fornecidos no arquivo `dados_ficha_a_desafio.csv`. O objetivo foi identificar problemas nos dados, propor soluÃ§Ãµes e realizar o tratamento dos mesmos utilizando **Python, SQL e DBT**.


## ğŸ“Š **Objetivos**
1. **ExploraÃ§Ã£o e AnÃ¡lise dos Dados**:
   - Identificar caracterÃ­sticas e padrÃµes dos dados.
   - Apontar possÃ­veis inconsistÃªncias e problemas.
   - Fazer inferÃªncias sobre a origem dos problemas.

2. **Tratamento e PadronizaÃ§Ã£o**:
   - Aplicar regras de limpeza e formataÃ§Ã£o dos dados.
   - Resolver inconsistÃªncias e normalizar valores.
   - Criar uma versÃ£o tratada e estruturada dos dados.

3. **ImplementaÃ§Ã£o no DBT**:
   - Criar um pipeline de transformaÃ§Ã£o utilizando **DBT (Data Build Tool)**.
   - Modularizar o cÃ³digo SQL para facilitar a manutenÃ§Ã£o e a reprodutibilidade.

---

## ğŸ› ï¸ Tecnologias Utilizadas
- **Python**: Para anÃ¡lise exploratÃ³ria dos dados, utilizando `pandas` e `numpy`
- **SQL**: Para consultas e manipulaÃ§Ã£o dos dados  
- **DBT (Data Build Tool)**: Para criaÃ§Ã£o e padronizaÃ§Ã£o dos modelos de dados  
- **PostgreSQL**: Banco de dados utilizado para armazenamento e transformaÃ§Ã£o  
- **Google Colab**: Para a execuÃ§Ã£o e documentaÃ§Ã£o da anÃ¡lise exploratÃ³ria  

---

## ğŸ“‚ Estrutura do DiretÃ³rio
```bash
desafio-dados/
â”œâ”€â”€ data/                              # Dados brutos e arquivos auxiliares
â”‚   â”œâ”€â”€ dados_ficha_a_desafio.csv      # Dataset original
â”‚   â”œâ”€â”€ descricao_de_campos.xlsx       # DescriÃ§Ã£o dos campos do dataset
â”‚
â”œâ”€â”€ dbt_project/                        # Projeto DBT para transformaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ models/                         # Modelos SQL criados para transformaÃ§Ã£o
â”‚   â”œâ”€â”€ dbt_project.yml                  # ConfiguraÃ§Ã£o do DBT
â”‚
â”œâ”€â”€ notebooks/                          # Notebooks para anÃ¡lise exploratÃ³ria
â”‚   â”œâ”€â”€ analise_exploratoria.ipynb      # Notebook com exploraÃ§Ã£o, analise e relatÃ³rios de desenvolvimento
â”‚
â”œâ”€â”€ scripts/                            # Scripts auxiliares para tratamento de dados
â”‚
â”œâ”€â”€ sql/                                # Scripts SQL antes e depois da transformaÃ§Ã£o
â”‚   â”œâ”€â”€ dados_brutos.sql                # Dump do banco antes da limpeza
â”‚   â”œâ”€â”€ dados_tratados.sql              # Dump do banco apÃ³s a limpeza e transformaÃ§Ã£o
â”‚
â”œâ”€â”€ README.md                           
â”œâ”€â”€ requirements.txt                     # DependÃªncias do projeto
â”œâ”€â”€ .gitignore                           
```

---

## ğŸ” AnÃ¡lise dos Dados
A anÃ¡lise exploratÃ³ria foi realizada utilizando Python e estÃ¡ documentada no notebook analise_exploratoria.ipynb. Caso queira visualizar o notebook, ele estÃ¡ disponÃ­vel no repositÃ³rio, e tambÃ©m Ã© possÃ­vel acessar o arquivo em formato PDF para consultar o relatÃ³rio de desenvolvimento deste projeto. Durante a anÃ¡lise, foram identificados diversos problemas nos dados, como:

âœ… **Valores inconsistentes em campos booleanos** (`0`, `1`, `True`, `False`)  
âœ… **Erros na padronizaÃ§Ã£o de datas** (`YYYY-MM-DD` vs `YYYY-MM-DD HH:MM:SS` vs `YYYY-MM-DD HH:MM:SS.SSS`)
âœ… **Dados categÃ³ricos com mÃºltiplas grafias** (ex.: `Motorista de TÃ¡xi` vs `Motorista de Carro de Passeio`)  
âœ… **PresenÃ§a de valores numÃ©ricos extremos e possivelmente errados** (ex.: `peso = 9.49 kg para um adulto`)  

A partir dessa anÃ¡lise, foram propostas estratÃ©gias para limpeza e padronizaÃ§Ã£o dos dados.

---

## ğŸ”„ TransformaÃ§Ã£o com DBT
A transformaÃ§Ã£o dos dados foi feita utilizando **SQL e DBT**, garantindo um modelo padronizado e pronto para futuras anÃ¡lises. As principais mudanÃ§as incluem:

ğŸ”¹ ConversÃ£o de valores booleanos para **0 e 1**  
ğŸ”¹ PadronizaÃ§Ã£o dos campos de data para formato Ãºnico  
ğŸ”¹ NormalizaÃ§Ã£o de categorias de ocupaÃ§Ã£o para evitar duplicidades  
ğŸ”¹ Ajuste de valores inconsistentes em variÃ¡veis numÃ©ricas  

O cÃ³digo SQL do modelo tratado pode ser encontrado em [`dbt_project/models].

---

## ğŸš€ Como Executar o Projeto

### ğŸ“Œ Requisitos
1. **Python** instalado (versÃ£o 3.8 ou superior)  
2. Banco de dados **PostgreSQL** configurado localmente  
3. **DBT** instalado (seguindo as instruÃ§Ãµes da documentaÃ§Ã£o oficial)  

### ğŸ“Œ Passos
1. Clone este repositÃ³rio:
   ```bash
   git clone [https://github.com/srtapacheco/desafio-dados]
   https://github.com/srtapacheco/desafio-dados
   cd desafio-dados
   ```
2. Instale as dependÃªncias:
   ```bash
   pip install -r requirements.txt
   ```
3. Configurar Banco de Dados
No arquivo `profiles.yml`, configure as credenciais do banco de dados:

```yaml
default:
  outputs:
    dev:
      type: postgres
      host: "localhost"
      user: "seu_usuario"
      password: "sua_senha"
      dbname: "desafio_db"
      schema: "public"
      threads: 1
  target: dev
```

4. Executar o Pipeline DBT
ApÃ³s configurar o banco, rode os seguintes comandos:

```bash
# Limpar cache do DBT
dbt clean

# Compilar os modelos
dbt compile

# Rodar o pipeline completo
dbt run --full-refresh
```

ApÃ³s a execuÃ§Ã£o, os dados tratados estarÃ£o disponÃ­veis na tabela `public.mart_dados_finais`. 

Para fins de teste e validaÃ§Ã£o, os arquivos de exportaÃ§Ã£o do banco (antes e depois da limpeza) estÃ£o disponÃ­veis na pasta sql/.

## ğŸ“Š **ValidaÃ§Ã£o dos Dados**
ApÃ³s a transformaÃ§Ã£o, realizamos comparaÃ§Ãµes entre os dados brutos e tratados para validar o pipeline. Algumas verificaÃ§Ãµes incluem:
- **ComparaÃ§Ã£o entre os valores antes e depois da transformaÃ§Ã£o**.
- **Contagem de registros antes e depois para evitar perda de dados inesperada**.
- **AnÃ¡lise de estatÃ­sticas descritivas para garantir a consistÃªncia dos dados corrigidos**.

---

## ğŸ“œ ConclusÃ£o
Este projeto demonstrou como uma **boa anÃ¡lise de dados** pode revelar problemas importantes e permitir a **transformaÃ§Ã£o estruturada** de um conjunto de dados, melhorando a performance, o armazenamento e a manipulaÃ§Ã£o desses dados, alÃ©m de garantir a veracidade e a coerÃªncia das informaÃ§Ãµes extraÃ­das dessas fontes de dados.
